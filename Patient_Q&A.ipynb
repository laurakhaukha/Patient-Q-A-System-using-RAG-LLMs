{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEP5ur7v5tBf"
   },
   "source": [
    "# NHS Patient Q&A  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate\n",
    "!pip install rouge_score\n",
    "!pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries/ packages used throughtout this project, please run prior to\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset\n",
    "from google.colab import files, drive\n",
    "import traceback\n",
    "from pathlib import Path #needed to improve procssing speed\n",
    "from sklearn.model_selection import train_test_split\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, EarlyStoppingCallback\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed if Google collab and the data is stored in local google drive:\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please note: Adapt the local directiory to run the subsequent script\n",
    "# Loading data from local directionary\n",
    "trainds = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/AMLH-NLP EHR/patient_qa_train.csv\",sep=\",\", dtype=str)\n",
    "testds = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/AMLH-NLP EHR/patient_qa_test.csv\",sep=\",\", dtype=str)\n",
    "NHS_documents_wd = Path(\"/content/drive/My Drive/Colab Notebooks/AMLH-NLP EHR/db_nhs_qa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-RETN47qC9j"
   },
   "source": [
    "## Section 1.0: Exploratory data anlysis (EDA) for provided data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA: Exploratory data analysis trainds\n",
    "print(f\"train df shape:{trainds.shape}: {trainds.columns}\")\n",
    "#EDA: Exploratory data analysis testds\n",
    "print(f\"test df shape:{testds.shape}: {testds.columns}\")\n",
    "\n",
    "# Looking at the disease column of the dataset to understand\n",
    "#what conditions the provided answers are related to\n",
    "disease_uniqu = trainds['disease'].unique()\n",
    "diagnosis_counts = trainds['disease'].value_counts()\n",
    "\n",
    "print(f\"Disease distribution: {diagnosis_counts}\")\n",
    "print(f\"Disease distribution: {disease_uniqu}\")  #Nr. of diseases discusesed 2392\n",
    "print(diagnosis_counts.head(20))\n",
    "#Observation diseases column do not only repsents diseases or conditions\n",
    "#but medical circumstances, alonside a seperator \"/\", which requires furhter cleaninign later on\n",
    "#the corresponding descriptions may be important later for the models context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descriptive statisics for display in the report Section 1.0: Introduction\n",
    "#Exploring the character lengths of both the answers and questions within the dataset\n",
    "trainds[\"question_length\"] = trainds[\"question\"].apply(lambda x: len(str(x).split()))\n",
    "trainds[\"answer_length\"] = trainds[\"answer\"].apply(lambda x: len(str(x).split()))\n",
    "#Exploring the descriptive stats for both answers and questions' characters:\n",
    "print(\"\\n Question word length stats:\")\n",
    "print(trainds[\"question_length\"].describe())\n",
    "print(\"\\n Answer word length stats:\")\n",
    "print(trainds[\"answer_length\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descriptive statisics for display in the report Section 1.0: Introduction\n",
    "#Exploring the character lengths of both the answers and questions within the dataset\n",
    "trainds[\"question_length\"] = trainds[\"question\"].apply(lambda x: len(str(x).split()))\n",
    "trainds[\"answer_length\"] = trainds[\"answer\"].apply(lambda x: len(str(x).split()))\n",
    "#Exploring the descriptive stats for both answers and questions' characters:\n",
    "print(\"\\n Question word length stats:\")\n",
    "print(trainds[\"question_length\"].describe())\n",
    "print(\"\\n Answer word length stats:\")\n",
    "print(trainds[\"answer_length\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKtOXtr9fIEf"
   },
   "source": [
    "## Section 2.0: Data Preprocessing (TF:IDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly, for the downstream implentaion of TF-IDF, the\n",
    "#Here the fetch_by_diagnosis function, was initally applied combined with safe_filename,\n",
    "#though the processing speed was immensly affected, as such a simple corpus build was decided for\n",
    "def safefilename(name):\n",
    "    \"\"\" Mapps the disease label to the matching NHS document in ./db_nhs_qa,Provided from the assignment brief, for \"\"\"\n",
    "    cle= re.sub(r'[^a-zA-Z0-9 ]', '_', name.strip().lower())\n",
    "    cle= re.sub(r'\\s+', '_', cle)\n",
    "    cle= re.sub(r'_+', '_', cle)\n",
    "    return cle +\".txt\"\n",
    "fi = {fn.name.lower(): fn for fn in NHS_documents_wd.iterdir() if fn.suffix == \".txt\"}\n",
    "\n",
    "#For this TF-IDF context, a corpus needs to be setup/build\n",
    "#loading/fetching all documents using the fetch_by_diagnosis function above\n",
    "#Herein only unqiue disease will be included, for a more valid evalution less\n",
    "corps_txt= { }\n",
    "miss= []\n",
    "for d in trainds[\"disease\"].unique():\n",
    "    dname = safefilename(d)\n",
    "    if dname in fi:\n",
    "        corps_txt[d] = fi[dname].read_text(encoding=\"utf-8\")\n",
    "    else:\n",
    "      miss.append(d)\n",
    "#Checking how many files were loaded in the corpus\n",
    "print(f\"{len(corps_txt)}, {len(miss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning: In preperation for TF:IDF Implementation, as outlined in Section 2.1.2\n",
    "# Text standarsiation, includes lowercasing, removing puncutation or furhter symbols like hyphens and /, whitepsecaes\n",
    "# Removal of stopwords is conducted later in the\n",
    "\n",
    "#Creating a copy of the used datasets, to enusure seperate and clean application for both models\n",
    "train_ds_idf_copy= trainds.copy()\n",
    "test_ds_idf_copy= testds.copy()\n",
    "\n",
    "#Creating a function for efficient, data cleaning (text standardisation),\n",
    "def standardisation_txt(txt_for_standard):\n",
    "  \"\"\" The following function ensures efficient cleaning of text,\n",
    "      hereby lowercasing, removing puncuation, hyphens and slashes  as well as unnesssary whitespaces: A step needed for TF:IDF \"\"\"\n",
    "  txt_for_standard= str(txt_for_standard)\n",
    "  txt_for_standard= txt_for_standard.lower()\n",
    "  txt_for_standard= re.sub(r\"[\\n\\t]\", ' ', txt_for_standard)\n",
    "  txt_for_standard= re.sub(r\"[-/]\", \" \", txt_for_standard)\n",
    "  txt_for_standard = txt_for_standard.translate(str.maketrans('', '', string.punctuation))\n",
    "  txt_for_standard = re.sub(r'\\s+', ' ', txt_for_standard).strip()\n",
    "  return txt_for_standard\n",
    "\n",
    "# Application of the standardisation_txt function, for the cleaning of the relvaent columns and documents and corpus\n",
    "# employed for TF-IDF implementation\n",
    "corps_IDF_ready_values = [standardisation_txt(doc) for doc in corps_txt.values()]\n",
    "corps_IDF_ready_keys = list(corps_txt.keys())\n",
    "train_ds_idf_copy[\"question\"]= train_ds_idf_copy[\"question\"].apply(standardisation_txt)\n",
    "train_ds_idf_copy[\"answer\"]= train_ds_idf_copy[\"answer\"].apply(standardisation_txt)\n",
    "test_ds_idf_copy[\"question\"]= test_ds_idf_copy[\"question\"].apply(standardisation_txt)\n",
    "test_ds_idf_copy[\"answer\"]= test_ds_idf_copy[\"answer\"].apply(standardisation_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove any leadin from '\\ufeff each document’s text\n",
    "corps_IDF_ready_values= [doc.lstrip('\\ufeff') for doc in corps_IDF_ready_values]\n",
    "corps_IDF_ready_keys= [doc.lstrip('\\ufeff') for doc in corps_IDF_ready_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vecotorising the corpus values: Addding the removal of stopwords via (stop_words='english'), part of precrocessing mentioned in report section 2.1.2\n",
    "vectoriser = TfidfVectorizer(stop_words='english',ngram_range=(1,2))\n",
    "# max_features=5000 was too harsh on the retrieval cutoff,\n",
    "#afftecting the cosine similarity score, hence why it was left out as a parameter\n",
    "corpsvect_ = vectoriser.fit_transform(corps_IDF_ready_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQtMhjm6vw57"
   },
   "source": [
    "## Section 3.0: TF-IDF Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementing the retk_docs, a function that implemtned TF-IDF for the document retrieval\n",
    "def retk_docs(question: str, k: int = 3):\n",
    "    \"\"\"Thsi function retruns the document with the highest ranking via (disease_label, snippet, score) tuples for `question`.\"\"\"\n",
    "    #Applying the standarsisation fuction from the code upstream,\n",
    "    # to clean the input question\n",
    "    stand_q = standardisation_txt(question)\n",
    "    #Vecorisation of the standardised input question\n",
    "    vexq   = vectoriser.transform([stand_q])\n",
    "    #Computing the cosine similarities against all docs\n",
    "    cosine_S  = cosine_similarity(vexq, corpsvect_).flatten()\n",
    "    #Filtering the question/docuemnt ouputs with the highest cosine_similarity scores\n",
    "    top_idxs = np.argsort(cosine_S)[::-1][:k]\n",
    "    #Storing the results in a list witht the question, corresponding document and cosien similiaryt score.\n",
    "    results = []\n",
    "    for idx in top_idxs:\n",
    "        question =corps_IDF_ready_keys[idx]\n",
    "        document =corps_IDF_ready_values[idx]\n",
    "        score = cosine_S[idx]\n",
    "        results.append((question, document, score))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMt3AF5HwtCm"
   },
   "source": [
    "##Section 4: TF-IDF MODEL Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7lVmo1cxb35"
   },
   "source": [
    "Section 4.1: Visual Inspection of Tf-IDF output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visiual inspection of some one results:\n",
    "egp_1 = test_ds_idf_copy.loc[0, \"question\"]\n",
    "egp_2 = test_ds_idf_copy.loc[6, \"question\"]\n",
    "\n",
    "#taking the best ranked outputs for display\n",
    "bestranked_ouputs = retk_docs(egp_1, k=3)\n",
    "print(f\"Input qustion: {egp_1} ?\")\n",
    "for rank, (question, document, score) in enumerate(bestranked_ouputs, 1):\n",
    "    print(f\"rank {rank}: {question} (score {score:.2f})\")\n",
    "    print(f\"NHS document: {document}\")\n",
    "\n",
    "#Looking throgh the next three examples with egp_2\n",
    "bestranked_ouputs = retk_docs(egp_2, k=3)\n",
    "print(f\"Input qustion: {egp_2}?\")\n",
    "for rank, (question, document, score) in enumerate(bestranked_ouputs, 1):\n",
    "    print(f\"Rank NR {rank}: {question} (score {score:.2f})\")\n",
    "    print(f\"NHS document: {document}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg= evaluate.load(\"rouge\")\n",
    "sm = SmoothingFunction().method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92uArEBFpEln"
   },
   "source": [
    "Section 4.2: Computing Performance metrics for TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function computing recall pformance at a specified\n",
    "def recallk(d, k=1):\n",
    "    \"\"\" Function calculcating the TF-IDF output, at a specified k(document ranking,\n",
    "        with 1 being top ranked document based on cosine similarity)  \"\"\"\n",
    "    cm = 0\n",
    "    pateint_ques= d[\"question\"].tolist()\n",
    "    true_labels = d[\"disease\"].tolist()\n",
    "    for question, true_label in zip(pateint_ques, true_labels):\n",
    "        t_k = [label for label, _, _ in retk_docs(question, k=k)]\n",
    "        if true_label in t_k:\n",
    "          cm  += 1\n",
    "    return cm/len(d)\n",
    "\n",
    "#applying the recallk function to calculate the recall pefromance at k1 and 3 (documents ranked)\n",
    "r_ranked1 = recallk(test_ds_idf_copy, k=1)\n",
    "r_ranked3 = recallk(test_ds_idf_copy, k=3)\n",
    "#Calculating the Precion,recall and f1 scors, treating dieases as classes\n",
    "yt= test_ds_idf_copy[\"disease\"].tolist()\n",
    "yp =[retk_docs(q, k=1)[0][0] for q in test_ds_idf_copy[\"question\"].tolist()]\n",
    "p1 =precision_score(yt, yp, average=\"micro\")\n",
    "r1c = recall_score(yt, yp, average=\"micro\")\n",
    "f1   = f1_score(yt, yp, average=\"micro\")\n",
    "#Inlcuding top retreived documents into the test set\n",
    "test_ds_idf_copy[\"tfidf_prediction\"] = [retk_docs(q, k=1)[0][1]  for q in test_ds_idf_copy[\"question\"]]\n",
    "#Computing the Rouge scores\n",
    "pred_ = test_ds_idf_copy[\"tfidf_prediction\"].tolist()\n",
    "ref_s  = test_ds_idf_copy[\"answer\"].tolist()\n",
    "rg_scored = rg.compute(predictions=pred_, references=ref_s,rouge_types=[\"rouge1\",\"rouge2\",\"rougeL\"],use_stemmer=True)\n",
    "#Computing the bleu scores\n",
    "bl_score = [sentence_bleu([ref.split()], pred.split(),weights=(.25,.25,.25,.25),smoothing_function=sm) for pred, ref in zip(pred_,ref_s)]\n",
    "mean_bleu = sum(bl_score) / len(bl_score)\n",
    "# Preparing the results for display analysis formmat\n",
    "tfidf_R = {\"Recall k1\": r_ranked1,\n",
    "           \"Recall k3\":r_ranked3,\n",
    "           \"Precision k1\": p1,\n",
    "           \"Recall (class 1)\": r1c,\n",
    "           \"F1 score (class 1)\": f1,\n",
    "           \"Rouge 1\": rg_scored[\"rouge1\"],\n",
    "           \"Rouge 2\": rg_scored[\"rouge2\"],\n",
    "           \"Rouge L\": rg_scored[\"rougeL\"],\n",
    "           \"Bleu-4\": mean_bleu}\n",
    "print(tfidf_R )\n",
    "#Downloading and printingthe results\n",
    "pd.DataFrame([tfidf_R]).to_csv(\"tfidfeval.csv\", index=False)\n",
    "files.download(\"tfidfeval.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmEgfmU5v3pj"
   },
   "source": [
    "## Section 5: T5-small pre-processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Please note: Adapt the local directiory to run the subsequent script\n",
    "#Reloading the datasets used for T5 fine-tuning\n",
    "train_t5_df = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/AMLH-NLP EHR/patient_qa_train.csv\",sep=\",\", dtype=str)\n",
    "test_t5_df = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/AMLH-NLP EHR/patient_qa_test.csv\",sep=\",\", dtype=str)\n",
    "T5_NHS_documents = Path(\"/content/drive/My Drive/Colab Notebooks/AMLH-NLP EHR/db_nhs_qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the model and its tokeniser\n",
    "t5_m = \"t5-small\"\n",
    "tokense_t5 = AutoTokenizer.from_pretrained(t5_m)\n",
    "model = T5ForConditionalGeneration.from_pretrained(t5_m)\n",
    "\n",
    "#Setting up t5s purpose as the model has different tasks/functions it can be set to:\n",
    "if t5_m  in [\"t5-small\"]:\n",
    "    prefix = \"question \"\n",
    "else:\n",
    "    prefix = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardsiging/cleaning the text for T5-fine tuning\n",
    "#lowecasing and removing white spaces\n",
    "def T5_standardisation_txt(T5txt_for_standard):\n",
    "  \"\"\" The following function ensures efficient cleaning of text,\n",
    "      hereby lowercasing and unnesssary whitespaces: A step needed for the T5 model\"\"\"\n",
    "  T5txt_for_standard= T5txt_for_standard.lower()\n",
    "  T5txt_for_standard= re.sub(r'\\s+', ' ', T5txt_for_standard).strip()\n",
    "  return T5txt_for_standard\n",
    "\n",
    "#Aplying this standardisation function to relavant columns\n",
    "train_t5_df[\"question_clean\"]= train_t5_df[\"question\"].apply(T5_standardisation_txt)\n",
    "train_t5_df[\"answer_clean\"]= train_t5_df[\"answer\"].apply(T5_standardisation_txt)\n",
    "train_t5_df[\"disease_clean\"] = train_t5_df[\"disease\"].apply(T5_standardisation_txt)\n",
    "# test_t5_df[\"question_clean\"]= test_t5_df[\"question\"].apply(T5_standardisation_txt) #Not apllied to improve model generalisaiton abbillty\n",
    "# test_t5_df[\"answer_clean\"]= test_t5_df[\"answer\"].apply(T5_standardisation_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_diseases = train_t5_df[\"disease_clean\"].unique()\n",
    "print(\"Nr unique diseases\", len(unique_diseases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_data_dir = T5_NHS_documents\n",
    "# Standardising file naames\n",
    "def safe_filename(name):\n",
    "    return re.sub(r'[^\\w\\-_. ]', '_', name).strip().replace(\" \", \"_\") + \".txt\"\n",
    "# Index all available .txt files in the folder\n",
    "file_index = {fn.name.lower(): fn for fn in T5_NHS_documents.iterdir() if fn.suffix == \".txt\"}\n",
    "#Creating the corpus and mapping each disease to the corresponding documents\n",
    "cor_txt = {}\n",
    "missed_desiease = []\n",
    "for disease in train_t5_df[\"disease_clean\"].unique():\n",
    "    fname = safe_filename(disease)\n",
    "    if fname in file_index:\n",
    "      cor_txt[disease]= file_index[fname].read_text(encoding=\"utf-8\")\n",
    "    else:\n",
    "        missed_desiease.append(disease)\n",
    "train_t5_df[\"NHS_docuemntsclean\"] = train_t5_df[\"disease_clean\"].map(cor_txt)\n",
    "\n",
    "# Creating the inputs for t5 in the appropriate dataframe\n",
    "train_t5_df[\"target_text\"] = train_t5_df[\"answer_clean\"]\n",
    "train_t5_df[\"input_text\"] = train_t5_df.apply(lambda row: f\"question: {row['question_clean']} context: {row['NHS_docuemntsclean']}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the training data into a 80/20 training, validation set split\n",
    "train_T5, val_T5 = train_test_split(train_t5_df[[\"input_text\", \"target_text\", \"disease_clean\"]], test_size=0.2, stratify=train_t5_df[\"disease_clean\"], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_doc_len = 512\n",
    "max_sum_length = 64\n",
    "prefix = \"question: \"\n",
    "\n",
    "def preprocesssamples(samples):\n",
    "    inputs = [prefix + doc for doc in samples[\"input_text\"]] #Adding the prefix here\n",
    "    # Tokenising the input\n",
    "    model_inputs = tokense_t5(inputs, max_length=max_doc_len, truncation=True, padding=\"max_length\")\n",
    "    #tokenising the labels\n",
    "    with tokense_t5.as_target_tokenizer():\n",
    "      targets = tokense_t5(text_target = samples[\"target_text\"], max_length=max_sum_length, truncation=True, padding=\"max_length\")\n",
    "      model_inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "      return model_inputs\n",
    "\n",
    "# Converting to hugging face dataset\n",
    "train_T5_ = Dataset.from_pandas(train_T5[[\"input_text\", \"target_text\"]].reset_index(drop=True))\n",
    "val_T5_ = Dataset.from_pandas(val_T5[[\"input_text\", \"target_text\"]].reset_index(drop=True))\n",
    "\n",
    "#Appplyign the preprocesssamples function\n",
    "token_train_T5_final = train_T5_.map(preprocesssamples, batched=True)\n",
    "token_val_T5_final = val_T5_.map(preprocesssamples, batched=True)\n",
    "print(token_train_T5_final[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TsQtPgWlJZmA"
   },
   "source": [
    "## Section 6: Fine-tuninig T5-small & Evaluating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #Manually swtiched to in google collab\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokense_t5, model=model)\n",
    "\n",
    "#Detailling the necessary training condtionons, justified and furhter dicussed in sections 2.1.3 & 2.2.2 in the correspoding report\n",
    "batchsize = 8\n",
    "trainargs = Seq2SeqTrainingArguments(output_dir=\"./t5_qa_model\",\n",
    "                                     eval_strategy = \"epoch\",\n",
    "                                     learning_rate=2e-5,\n",
    "                                     per_device_train_batch_size=batchsize,\n",
    "                                     per_device_eval_batch_size=batchsize,\n",
    "                                     weight_decay=0.01,\n",
    "                                     save_total_limit=2,\n",
    "                                     num_train_epochs=6,\n",
    "                                     predict_with_generate=True,\n",
    "                                     push_to_hub=False,\n",
    "                                     report_to=\"none\",\n",
    "                                     logging_dir=\"./logs\",\n",
    "                                     logging_steps=10,\n",
    "                                     save_strategy=\"epoch\",\n",
    "                                     metric_for_best_model=\"eval_loss\",\n",
    "                                     greater_is_better=False,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3r52uL6Ik7hF"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up & training th t5 small model, thereby evaluation metrics as well\n",
    "t5trainer= Seq2SeqTrainer(\n",
    "    model_init=model_init,\n",
    "    args=trainargs,\n",
    "    train_dataset=token_train_T5_final,\n",
    "    eval_dataset=token_val_T5_final,\n",
    "    tokenizer=tokense_t5,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)])\n",
    "t5trainer.train() #training the model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visulalising the Loss overtime during training per epoch, for the trainign and validation set\n",
    "epoc = [1, 2, 3, 4, 5, 6]\n",
    "obtained_trainloss = [1.545600, 1.464400, 1.466400, 1.440500, 1.678800, 1.453500]\n",
    "obtained_vlalloss = [1.481636, 1.444403, 1.419544, 1.408478, 1.402640, 1.399771]\n",
    "#Customising and plotting the training versus validation loss\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epoc,obtained_trainloss, marker='o',color='blue',label='T5-Small Training Loss')\n",
    "plt.plot(epoc,obtained_vlalloss, marker='o', color='pink', label=' T5-Small Validation Loss')\n",
    "plt.xlabel('Number of Training Epochs Conducted')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.title('Training vs Validation Loss of fine-tuned T5-Small over 6 Epochs')\n",
    "plt.xticks(epoc)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainargs.output_dir) # Checking where the model was put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore = evaluate.load(\"bertscore\") # loading the bertscore\n",
    "rg = evaluate.load(\"rouge\") #loading rouge metics and its correpsonding\n",
    "smooth= SmoothingFunction().method1 #smoothing feature for the BLEU sores\n",
    "\n",
    "# # #Defining an evaluation functions which computes the tokenised precicion, recall, F1score metrics\n",
    "# # #alongside the bleu and rouge metrics, which later will be computed in the training loop\n",
    "def comp_mt(el_pd):\n",
    "    preds_, labels_ =el_pd\n",
    "    #Ensuring preds_, labels_ are in numpy format and on cpu\n",
    "    if isinstance(preds_, torch.Tensor):\n",
    "        preds_ =preds_.cpu().numpy()\n",
    "    if isinstance(labels_, torch.Tensor):\n",
    "        labels_ =labels_.cpu().numpy()\n",
    "    #Due to OverflowError in the past, replacing -100 labels with token IDs,avoiding decoding issuees\n",
    "    #Since Pytorch ignored these indexes\n",
    "    labels_ =np.where(labels_ != -100, labels_, tokense_t5.pad_token_id)\n",
    "    try:\n",
    "        #Due to ongoing OverflowErrors error enountered during evalution, the intseq was created\n",
    "        #Converiting sequences of tokenids into integers via intseq, avoiding decoding issues encountered earlier\n",
    "        def intseq(sq):\n",
    "            sq = [int(min(max(tok, 0), 2**31 - 1)) for tok in list(sq)] if hasattr(sq, '__iter__') else [int(min(max(sq, 0), 2**31 - 1))]\n",
    "            return sq\n",
    "        #Decoding the tokens into strings for evaluation\n",
    "        deco_preds_ =tokense_t5.batch_decode([safe_int_seq(seq) for seq in preds_], skip_special_tokens=True)\n",
    "        decod_labels_ =tokense_t5.batch_decode([safe_int_seq(seq) for seq in labels_], skip_special_tokens=True)\n",
    "        #tokensing each word, and flattening for evaluation\n",
    "        pd_tok=[gen.split() for gen in deco_preds_]\n",
    "        ref_tok=[ref.split() for ref in decod_labels_]\n",
    "        ypred = [token for sent in pd_tok for token in sent]\n",
    "        y_ref = [token for sent in ref_tok for token in sent]\n",
    "        #truncating for alignment between both the refernce and prediciotns\n",
    "        minlength=min(len(y_ref), len(ypred))\n",
    "        ypred = ypred[:minlength]\n",
    "        y_ref =y_ref[:minlength]\n",
    "        #Computing precision, recall, accuracy and the F-1 score, each for word-token comparison between reference and prediction\n",
    "        tokprec_=precision_score(y_ref, ypred, average=\"micro\", zero_division=0)\n",
    "        tokrecall =recall_score(y_ref, ypred, average=\"micro\", zero_division=0)\n",
    "        tokf1_= f1_score(y_ref, ypred, average=\"micro\", zero_division=0)\n",
    "        tokacc =accuracy_score(y_ref, ypred)\n",
    "        #Computing rouge, blue and bert metrics for evaluation\n",
    "        rgsc =rg.compute(predictions=deco_preds_, references=decod_labels_, use_stemmer=True)\n",
    "        blsc= [sentence_bleu([ref], pred, weights=(.25, .25, .25, .25), smoothing_function=smooth)for pred, ref in zip(pd_tok, ref_tok)]\n",
    "        blemean= np.mean(blsc)\n",
    "        bert_=bertscore.compute(predictions=deco_preds_, references=decod_labels_, lang=\"en\")\n",
    "        meanbertscore_= round(np.mean(bert_[\"f1\"]), 3)\n",
    "        return {\n",
    "            \"Precision (token)\":tokprec_,\n",
    "            \"recall(token)\":tokrecall,\n",
    "            \"F1(token)\":tokf1_,\n",
    "            \"accuracy(token)\":tokacc,\n",
    "            \"roge1\": rgsc[\"rouge1\"],\n",
    "            \"rouge2\":rgsc[\"rouge2\"],\n",
    "            \"rougeL\":rgsc[\"rougeL\"],\n",
    "            \"bleu4\": blemean,\n",
    "            \"BERTScore-F1\":meanbertscore_}\n",
    "    except Exception as e:   #this was neccessary for debugging as OverflowErrors kept on reappearing\n",
    "        print(\"compt_ct failed\")\n",
    "        traceback.print_exc() #\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/content/drive/My Drive/Colab Notebooks/AMLH-NLP EHR/model_trained2/model_trained_weights/checkpoint-14406\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applytig the Evaluation Fucntion to the trained model, from the specified\n",
    "t5trainer= Seq2SeqTrainer(model= model, args=trainargs,tokenizer=tokense_t5,data_collator=data_collator,eval_dataset=token_val_T5_final)\n",
    "#Making model predctions\n",
    "preds = t5trainer.predict(token_val_T5_final)\n",
    "#Verying their shape\n",
    "print(preds.predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Applyign the evlation function from above to here\n",
    "met = comp_mt((preds.predictions, preds.label_ids))\n",
    "# #Loop/priting through the results\n",
    "if met is not None: #Since metrics were previously empty due to bug in comp_mt\n",
    "    for k, v in met.items():\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eibGtx0GLdgV"
   },
   "source": [
    "## Section 6: RAG T5-small via TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty lists for storage for evalution results\n",
    "gen_ans=[]\n",
    "ref_ans=[]\n",
    "ques=test_t5_df[\"question\"].tolist()\n",
    "ref= test_t5_df[\"answer\"].tolist()\n",
    "m_ax= 200\n",
    "gen_max = 100\n",
    "\n",
    "#Having already evaluted the results, it was decided to truncate the context\n",
    "def t_text(text, maxmokens=200):\n",
    "    t = text.split()\n",
    "    return \" \".join(t[:maxmokens])\n",
    "#Initialising loop to incoperate TF-IDF into T5, prodcuing an ouptut\n",
    "for i in range(len(ques)):\n",
    "    question = ques[i]\n",
    "    reference = ref[i]\n",
    "    #Using TF-IDF function from section 5.0\n",
    "    tdc = retk_docs(question, k=1)[0][1]\n",
    "    cont = t_text(tdc, m_ax)\n",
    "    #Creating the T5 necessary input prompt\n",
    "    t5_input = f\"Question: {question} context: {cont}\"\n",
    "    #Tokenining and using t5 and IF-IDF\n",
    "    input_ids = tokense_t5([t5_input], return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(**input_ids, max_new_tokens=gen_max, num_beams=4,early_stopping=True, no_repeat_ngram_size=3)\n",
    "    gen = tokense_t5.decode(output_ids[0], skip_special_tokens=True)\n",
    "    #appeding the generated results into the empty lists\n",
    "    gen_ans.append(gen)\n",
    "    ref_ans.append(reference)\n",
    "#Prining the results\n",
    "print(f\"output,answer{gen_ans},ref answer{ref_ans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkoAdDQTLjOg"
   },
   "source": [
    "## Section 7: T5-small RAG: Final Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensuring the inputs are strings for subsequent evalution\n",
    "gen_ans = [str(x) for x in gen_ans]\n",
    "ref_ans = [str(x) for x in ref_ans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg = evaluate.load(\"rouge\") #loading rouge metics and its correpsonding\n",
    "sm= SmoothingFunction().method1 #smoothing feature for the BLEU sores\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "#Computing the Rouge scores for the predicted againt refefences sets\n",
    "rg_= rg.compute(predictions=gen_ans, references=ref_ans,  rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"],use_stemmer=True)\n",
    "#Computing the BLEU-4 scores for the predicited agaisn the reference sets\n",
    "bleuscore_ =[sentence_bleu([ref.split()], gen.split(), weights=(.25, .25, .25, .25), smoothing_function=sm) for gen, ref in zip(gen_ans, ref_ans)]\n",
    "meanbleu_ = np.mean(bleuscore_) #computing mean blue score: meanbleu_\n",
    "#Computing the BERTScore-F1\n",
    "bert_result = bertscore.compute(predictions=gen_ans, references=ref_ans, lang=\"en\")\n",
    "avg_bertscore_f1 = round(np.mean(bert_result[\"f1\"]), 3)\n",
    "\n",
    "#token level evaltuation scores\n",
    "pd_tok= [gen.split() for gen in gen_ans]\n",
    "ref_tok= [ref.split() for ref in ref_ans]\n",
    "ypred =[token for sent in pd_tok for token in sent]\n",
    "y_ref =[token for  sent in ref_tok for token in sent]\n",
    "minlen = min(len(y_ref), len(ypred))\n",
    "ypred = ypred[:minlen]\n",
    "y_ref = y_ref[:minlen]\n",
    "\n",
    "#Computing precision, recall, f1 and exact match accuracy scores for the tokenised text evaluation\n",
    "tokprec_ = precision_score(y_ref, ypred, average=\"micro\", zero_division=0)\n",
    "tokrecall= recall_score(y_ref, ypred, average=\"micro\", zero_division=0)\n",
    "tokf1_ = f1_score(y_ref, ypred, average=\"micro\", zero_division=0)\n",
    "tokacc = accuracy_score(y_ref, ypred)\n",
    "\n",
    "#Compilling the evaluation metrics results\n",
    "t5ragere = {\"rouge 1\":rg_[\"rouge1\"], \"rouge 2\":rg_[\"rouge2\"],\n",
    "            \"rouge L\": round(rg_[\"rougeL\"], 3),\"bleu\":meanbleu_,\n",
    "            \"bertscore-F1\": avg_bertscore_f1, \"precision(Token)\":tokprec_,\n",
    "            \"recall Token)\": round(tokrecall, 3), \"F1 token_\": tokf1_, \"accuracy(Token)\":tokacc}\n",
    "print(t5ragere)\n",
    "#Saving the results into local dir\n",
    "pd.DataFrame([t5ragere]).to_csv(\"T5rag.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
